{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Data from WeRateDogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data\n",
    "\n",
    "Data has been collected from three different sources:\n",
    "\n",
    "* Main data\n",
    "    - Already provided by Udacity as a local csv file.\n",
    "    - 2356 WeRateDogs tweets have been loaded into one dataframe.\n",
    "    - we need to go for two other sources to gather more data.\n",
    "* Data relative to the breed of dog:\n",
    "    - Image prediction has been programmatically downloaded from [cloudfront](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv).\n",
    "    - From the tsv file, 2075 image prediction has been loaded into one dataframe.\n",
    "* Data about retweet and favorite\n",
    "    - those data are available from Twitter API. That require to create a tweeter developper account in order to generate some credentials that can be use to programatically query the Twitter API.\n",
    "    - 2337 records has been retrieved then saved as a file 'tweet_json.txt'\n",
    "  \n",
    "The **gather** step get concluded by having three files stored locally, each one getting loaded in its own dataframe so that we can start assessing what we got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Data\n",
    "We start loading each data file into its own dataframe by executing three simple line of code:\n",
    "\n",
    "```\n",
    "df_twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "df_image_predictions = pd.read_csv('image-predictions.tsv', sep='\\t')\n",
    "df_tweet_data = pd.read_csv('tweet_json.txt')\n",
    "```\n",
    "\n",
    "Then we perform two assessments:\n",
    "\n",
    "* visual one that give glimpse over quality and tidiness issue\n",
    "* programmatic one that can confirm and also complete what has been visually assess.\n",
    "\n",
    "### Visual assessment\n",
    "\n",
    "Each dataset has been visually assess by just sampling it.\n",
    "\n",
    "Here are the findings:\n",
    "\n",
    "* specific to **Twitter Archive**:\n",
    "    - The format of values for columns `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` and `timestamp` indicate that there's a problem of type (which would be clarified via programmatic assessemnt)\n",
    "    - Column `name` has some values to \"None\" which probalby means that the dog's name is not known => we should get null instead of \"None\" for those\n",
    "    - In columns `doggo`, `floofer`, `pupper` and `puppo` 'None' is displayed for null and when not null the value is the same as the column name. This looks like a tidiness issue: Each variable forms a column. Here the dog stage variable is spread over four columns.\n",
    "    - Column `source` contains markup making the reading less friendly.\n",
    "    - The presence of the colum `retweeted_status_id`, `retweeted_status_user_id` and `retweeted_status_timestamp` and the fact that they sometimes contain not null value indicate that there are record related to retweeted tweets which are not in the population we want analyze.\n",
    "* **General tidiness issue**:\n",
    "    - The three datasets should be combined into one since they are all part of the same observation unit.\n",
    "\n",
    "### Programmatic assessment\n",
    "\n",
    "For this we use the common method like `pandas.Dataframe.info()`, `pandas.Dataframe.dtypes`,  `pandas.Dataframe.describes()` and `pandas.Series.value_counts()` plus more specific one when needed.\n",
    "\n",
    "Assessing the **Twitter Archive**:\n",
    "\n",
    "* **Type issues**:\n",
    "    - Following columns should be `int64` or `string` instead `float64`: \n",
    "      - in_reply_to_status_id\n",
    "      - in_reply_to_user_id\n",
    "      - retweeted_status_id\n",
    "      - retweeted_status_user_id\n",
    "    - Following colums should be of type `datetime` instead of `object`:\n",
    "      - timestamp\n",
    "      - retweeted_status_timestamp\n",
    "    - Column `source` should be category\n",
    "* **Value issues**:\n",
    "    - `rating_denominator` should always be 10.\n",
    "    \n",
    "Assessing the **prediction**:   \n",
    "\n",
    "* 281 records are missing when compare to the twitter archive.\n",
    "* some records concern retweet.\n",
    "\n",
    "Assessing the **json**:\n",
    "\n",
    "* 19 records are missing (those are the failed calls to the Twitter API while getting the tweet info.\n",
    "\n",
    "### Assessment summary\n",
    "\n",
    "#### Quality issues\n",
    "\n",
    "* Common dataset:\n",
    "    - there are records related to retweeted tweet\n",
    "    - missing record in **picture prediction** and **tweet data** datasets\n",
    "* **twitter achive dataset**:\n",
    "    - Following columns should be int64 or string instead float64: `in_reply_to_status_id`, `in_reply_to_user_id`\n",
    "    - Following columns are relative to retweeted and so should be removed: `retweeted_status_id`, `retweeted_status_user_id` and `retweeted_status_timestamp`\n",
    "    - Column `timestamp` should be of type datetime instead of object\n",
    "    - Column `name` contain invalid values (ex.: \"None\" instead of `Nan` when null)\n",
    "    - Column `source` should be a category.\n",
    "    - Column `source` should be friendly (remove markup).\n",
    "    - Column `rating_denominator` contains values other that \"10\"\n",
    "    - Column `rating_numerator` have few cases of missing decimals that can be retrieved from the column `text`.\n",
    "    \n",
    "    \n",
    "#### Tidiness issues\n",
    "\n",
    "* The three datasets should be combined into one since they are all part of the same observation unit.\n",
    "* In **Twitter archive** the columns `doggo`, `floofer`, `pupper` and `puppo` are in fact the same variable and so should be combined into one column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "We start by making a copy of each dataframe by running:\n",
    "```\n",
    "df_twitter_archive_clean = df_twitter_archive.copy()\n",
    "df_image_predictions_clean = df_image_predictions.copy()\n",
    "df_tweet_data_clean = df_tweet_data.copy()\n",
    "``` \n",
    "\n",
    "Then we clean two tidiness issues and eight quality issues. For each one we follow the same pattern: define, code then test. The details of those steps are present in the **wrangle_act.ipynb**\n",
    "\n",
    "### Tidiness\n",
    "\n",
    "The main tidiness issue is that all the three datasets correspond to the same observation unit. In fact we just get **prediction** and **tweet** info to enrich our dataset with information relative to breed and tweet popularity (retweet count and favorite count). `pandas.merge()` make this easy to solve and the test step is simply done by checking the output of `pandas.Dataframe.info()`.\n",
    "\n",
    "The other tidiness issue is that the dog stage variable is spread on four columns when it should be one. `pandas.melt()` solves most of the issue, however few cases remain of multiple stages, in that case the value in the newly melted column is updated by concatenating all the stages into one csv string value. By checking `pandas.Dataframe.info()` and `pandas.Series.value_counts()` on the newly created column, we can assert that the issue is succesfully fiexed. \n",
    "\n",
    "The reason why I prefere starting to solve tidiness before quality issue is because I know that I would have to remove records relative to retweeted tweet. Better performing that removal once than twice or three times. So now that the obervation unit issue is fixed by having one dataframe, I only have to perform the cleaning of retweeted once.\n",
    "\n",
    "### Quality\n",
    "\n",
    "Without a surprise the first quality issue to be fixed is the removal of retweeted. If **retweeted_status_id** is not null this means that record is a retweet, that assertion is use at coding time to perform the removal and at test time for checking the correction.\n",
    "\n",
    "I won't go into the details of each issue. The \"Define\" step is mainly a rephrase of the assessment summary. \"Code\" has been achieve easily with the pandas API and \"Test\" use common descriptive method such as `pandas.Dataframe.info()` or `pandas.Series.value_counts()`. The only issue that worth to be mentioned is cleaning the dog name. Programmatically this does not present any challenge. The only thing is that there's no clean pattern to fix. Decided to remove name with a length less than 2 characters is arbitraty. There are wrong names of 3 char and there are also valid names of 2 char too (like 'Bo'). This should be mentioned along with any analytical conclusion drawn from the variable **name**.\n",
    "\n",
    "### Finally\n",
    "\n",
    "Once every thing is clean, the result data set is save in a file for analysis. As a first step of analysis, we should keep in mind that saving to csv then loading from it will drop the transtyping we may have done (to Datetime for example).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
